ðŸ§  Bigram Language Model with PyTorch ðŸ”¥
A simple yet powerful character-level language model inspired by Andrej Karpathy's "nanoGPT" â€” built from scratch using PyTorch. This project demonstrates how neural networks can learn to generate text one character at a time, using just bigram (2-character) relationships!

ðŸ”§ Whatâ€™s Inside
âœ… Clean data preprocessing with character-to-index encoding

âœ… Minimal neural network architecture with a trainable embedding table

âœ… Custom training loop with best-loss tracking and model checkpointing

âœ… Text generation function that continues a prompt character by character

âœ… Fully commented, beginner-friendly code for educational use

ðŸ“˜ How It Works
Reads a plain-text dataset and splits it into train/validation sets

Encodes the text into integers (vocabulary of unique characters)

Trains a neural network to predict the next character from the previous one

Generates new text based on a seed like "hello" or a single character

ðŸš€ Perfect For:
Beginners learning PyTorch

Understanding how language models work at a low level

Experimenting with embeddings and token prediction

ðŸ“‚ Files Included:
bigram_dataprep.py â€“ handles encoding, decoding, and batching

bigram_NN.py â€“ builds, trains, and evaluates the bigram neural network
